# Spark home
hibench.spark.home /usr/local/spark

# Spark master
#   standalone mode: spark://xxx:7077
#   YARN mode: yarn-client
hibench.spark.master yarn

# executor number and cores when running on Yarn
hibench.yarn.executor.num 35
hibench.yarn.executor.cores 21

# executor and driver memory in standalone & YARN mode
spark.executor.memory 28G
spark.driver.memory 16G

#spark.yarn.am.cores 2
#spark.eventLog.dir = hdfs://ambari-arm-server:8020/
#spark.eventLog.enabled = true
#spark.memory.useLegacyMode = true
#spark.storage.memoryFraction = 0.8

#spark.driver.extraJavaOptions = -Dlog4j.configuration=file:/opt/client/Spark2x/spark/conf/log4j.properties -Djetty.version=x.y.z -Dorg.xerial.snappy.tempdir=/opt/client/Spark2x/tmp

#spark.executor.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=./log4j-executor.properties

#spark.driver.extraClassPath = /opt/client/Spark2x/spark/conf/:/opt/client/Spark2x/spark/jars/*:/opt/client/Spark2x/spark/examples/jars/*

# set spark parallelism property according to hibench's parallelism value
spark.default.parallelism     ${hibench.default.map.parallelism}

# set spark sql's default shuffle partitions according to hibench's parallelism value
spark.sql.shuffle.partitions  ${hibench.default.shuffle.parallelism}

#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true


spark.driver.extraLibraryPath /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.eventLog.dir hdfs:///spark2-history/
spark.eventLog.enabled true

spark.executor.extraLibraryPath /usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 7d
spark.history.fs.cleaner.maxAge 90d
spark.history.fs.logDirectory hdfs:///spark2-history/
spark.history.kerberos.keytab none
spark.history.kerberos.principal none
spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider
spark.history.store.path /var/lib/spark2/shs_db
spark.history.ui.port 18081
#spark.io.compression.lz4.blockSize 128kb
spark.master yarn-client
#spark.shuffle.file.buffer 1m
#spark.shuffle.io.backLog 8192
#spark.shuffle.io.serverThreads 128
#spark.shuffle.unsafe.file.output.buffer 5m
#spark.sql.autoBroadcastJoinThreshold 26214400
spark.sql.hive.convertMetastoreOrc true
spark.sql.hive.metastore.jars /usr/hdp/current/spark2-client/standalone-metastore/*
#spark.sql.hive.metastore.version 3.0
spark.sql.orc.filterPushdown true
spark.sql.orc.impl native
spark.sql.statistics.fallBackToHdfs true
spark.sql.warehouse.dir /apps/spark/warehouse
spark.unsafe.sorter.spill.reader.buffer.size 1m
spark.yarn.historyServer.address 127.0.0.1:18081
spark.yarn.queue default

spark.shuffle.compress true
spark.rdd.compress true
spark.io.compression.codec lz4
spark.shuffle.spill.compress true
spark.locality.wait 10s
spark.memory.fraction 0.56
spark.yarn.executor.memeoryOverhead=20148
spark.shuffle.file.buffer 64k
spark.reducer.maxSizeInFlight   96m
spark.shuffle.io.maxRetries  60
spark.shuffle.io.retryWait  60s
spark.shuffle.memoryFraction 0.4 
spark.storage.memoryFraction 0.4
#spark.shuffle.memoryFraction  0.3
#spark.executor.extraJavaOptions  -XX:+UseNUMA -XX:ParallelGCThreads=15 -Xms14g
#spark.driver.extraJavaOptions  -XX:+UseNUMA -XX:ParallelGCThreads=15
#spark.executor.extraJavaOptions  -XX:+UseNUMA
#spark.driver.extraJavaOptions  -XX:+UseNUMA
###################%%%%%%%%%%%#############################
#spark.executor.extraJavaOptions -XX:+UseNUMA -XX:BoxTypeCachedMax=100000  -XX:+UnlockExperimentalVMOptions -XX:+EnableIntrinsicExternal -XX:+UseF2jBLASIntrinsics
#spark.yarn.am.extraJavaOptions -XX:+UseNUMA -XX:BoxTypeCachedMax=100000 -XX:+UnlockExperimentalVMOptions -XX:+EnableIntrinsicExternal -XX:+UseF2jBLASIntrinsics
spark.executor.extraJavaOptions -XX:+UseNUMA -XX:BoxTypeCachedMax=100000 -XX:+UnlockExperimentalVMOptions -XX:+EnableIntrinsicExternal -XX:+UseF2jBLASIntrinsics -Xms14g -XX:ParallelGCThreads=12
spark.yarn.am.extraJavaOptions -XX:+UseNUMA -XX:BoxTypeCachedMax=100000 -XX:+UnlockExperimentalVMOptions -XX:+EnableIntrinsicExternal -XX:+UseF2jBLASIntrinsics
